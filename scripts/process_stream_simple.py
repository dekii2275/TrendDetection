from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col,
    from_json,
    from_unixtime,
    to_timestamp,
    window,
    collect_list,
    date_format,
    count,
)
from pyspark.sql.types import StructType, StringType, LongType
import subprocess
import json

# Schema stream
schema = (
    StructType()
    .add("title", StringType())
    .add("timestamp", LongType())
    .add("topic", StringType())
)

spark = (
    SparkSession.builder.appName("SimpleTrendingNews")
    .config("spark.sql.shuffle.partitions", "2")  # Giáº£m partition
    .getOrCreate()
)

spark.sparkContext.setLogLevel("WARN")

print("ğŸš€ Báº¯t Ä‘áº§u Spark Streaming...")

# READ FROM KAFKA STREAM
df = (
    spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers", "localhost:9092")
    .option("subscribe", "news_raw")
    .option("startingOffsets", "latest")
    .option("failOnDataLoss", "false")
    .load()
)

# Parse JSON
parsed = (
    df.selectExpr("CAST(value AS STRING)")
    .select(from_json(col("value"), schema).alias("data"))
    .select("data.*")
)

# Chuáº©n hÃ³a timestamp - timestamp Ä‘Ã£ á»Ÿ dáº¡ng milliseconds
parsed_with_time = parsed.withColumn("event_time", to_timestamp(from_unixtime(col("timestamp") / 1000)))

# Filter dá»¯ liá»‡u há»£p lá»‡
filtered = parsed_with_time.filter(
    col("event_time").isNotNull() & 
    col("title").isNotNull() & 
    col("topic").isNotNull()
)

# Windowed aggregation Ä‘Æ¡n giáº£n
windowed = (
    filtered.withWatermark("event_time", "5 minutes")
    .groupBy(
        window(col("event_time"), "10 minutes"),  # Window 10 phÃºt
        col("topic"),
    )
    .agg(
        collect_list("title").alias("docs"),
        count("*").alias("doc_count")
    )
    .select(
        col("topic"),
        col("docs"),
        col("doc_count"),
        date_format(col("window.start"), "yyyy-MM-dd HH:mm:ss").alias("win_start"),
        date_format(col("window.end"), "yyyy-MM-dd HH:mm:ss").alias("win_end"),
    )
    .filter(col("doc_count") >= 2)  # Chá»‰ cáº§n 2 bÃ i trá»Ÿ lÃªn
)

def simple_process_window(batch_df, batch_id):
    print(f"\nğŸ”¥ === BATCH {batch_id} ===")
    
    if batch_df.count() == 0:
        print("â†’ Batch rá»—ng, bá» qua")
        return
    
    rows = batch_df.collect()
    print(f"â†’ CÃ³ {len(rows)} nhÃ³m topic")
    
    for row in rows:
        topic = row["topic"]
        docs = row["docs"]
        doc_count = row["doc_count"]
        win_start = row["win_start"]
        win_end = row["win_end"]

        print(f"\nğŸ“° Topic: {topic}")
        print(f"ğŸ“Š Sá»‘ bÃ i: {doc_count}")
        print(f"â° Window: {win_start} â†’ {win_end}")
        
        if doc_count >= 2:
            print("âœ… Äá»§ Ä‘iá»u kiá»‡n! Sample titles:")
            for i, title in enumerate(docs[:3]):
                print(f"   {i+1}. {title}")
            
            # CÃ³ thá»ƒ gá»i generative_topic.py á»Ÿ Ä‘Ã¢y náº¿u cáº§n
            # print("â†’ [Sáº½ gá»i generative_topic.py]")
        else:
            print("â†’ KhÃ´ng Ä‘á»§ bÃ i, bá» qua")

# Start streaming
query = (
    windowed.writeStream
    .foreachBatch(simple_process_window)
    .option(
        "checkpointLocation",
        "/home/lok/dev/projects/Parallel_computing/TrendDetection/scripts/checkpoint_simple"
    )
    .outputMode("update")
    .trigger(processingTime='30 seconds')  # Trigger má»—i 30 giÃ¢y
    .start()
)

print("ğŸ“¡ Streaming Ä‘ang cháº¡y... Nháº¥n Ctrl+C Ä‘á»ƒ dá»«ng")
try:
    query.awaitTermination()
except KeyboardInterrupt:
    print("\nğŸ›‘ Dá»«ng streaming...")
    query.stop()
    spark.stop()