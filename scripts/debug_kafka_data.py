#!/usr/bin/env python3
"""
Script debug Ä‘á»ƒ kiá»ƒm tra dá»¯ liá»‡u tá»« Kafka vÃ  timestamp formatting
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col,
    from_json,
    from_unixtime,
    to_timestamp,
    date_format,
)
from pyspark.sql.types import StructType, StringType, LongType
import time

# Schema giá»‘ng vá»›i process_stream.py
schema = (
    StructType()
    .add("title", StringType())
    .add("timestamp", LongType())
    .add("topic", StringType())
)

spark = (
    SparkSession.builder.appName("DebugKafkaData")
    .config("spark.sql.shuffle.partitions", "4")
    .getOrCreate()
)

spark.sparkContext.setLogLevel("WARN")

print("ğŸ” Debug: Äá»c dá»¯ liá»‡u tá»« Kafka...")

# Äá»c má»™t batch dá»¯ liá»‡u tá»« Kafka (khÃ´ng pháº£i stream)
try:
    df = (
        spark.read.format("kafka")
        .option("kafka.bootstrap.servers", "localhost:9092")
        .option("subscribe", "news_raw")
        .option("startingOffsets", "earliest")
        .option("endingOffsets", "latest")
        .load()
    )
    
    print(f"ğŸ“Š Tá»•ng sá»‘ message trong Kafka: {df.count()}")
    
    # Parse JSON
    parsed = (
        df.selectExpr("CAST(value AS STRING)")
        .select(from_json(col("value"), schema).alias("data"))
        .select("data.*")
    )
    
    print("ğŸ“‹ Schema cá»§a dá»¯ liá»‡u Ä‘Ã£ parse:")
    parsed.printSchema()
    
    print("ğŸ“‹ Sample dá»¯ liá»‡u:")
    parsed.show(5, truncate=False)
    
    # Test timestamp conversion
    normalized_ts = col("timestamp") / 1000
    parsed_with_time = parsed.withColumn("event_time", to_timestamp(from_unixtime(normalized_ts)))
    
    print("ğŸ“‹ Dá»¯ liá»‡u sau khi convert timestamp:")
    parsed_with_time.select("title", "timestamp", "event_time", "topic").show(5, truncate=False)
    
    # Kiá»ƒm tra phÃ¢n bá»‘ theo topic
    print("ğŸ“Š PhÃ¢n bá»‘ theo topic:")
    parsed_with_time.groupBy("topic").count().show()
    
    # Kiá»ƒm tra pháº¡m vi thá»i gian
    print("ğŸ“Š Pháº¡m vi thá»i gian:")
    parsed_with_time.select(
        date_format("event_time", "yyyy-MM-dd HH:mm:ss").alias("formatted_time")
    ).agg(
        {"formatted_time": "min"},
        {"formatted_time": "max"}
    ).show()

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c tá»« Kafka: {e}")
    print("ğŸ’¡ HÃ£y kiá»ƒm tra:")
    print("   - Kafka server cÃ³ Ä‘ang cháº¡y khÃ´ng?")
    print("   - Topic 'news_raw' cÃ³ tá»“n táº¡i khÃ´ng?")
    print("   - CÃ³ dá»¯ liá»‡u trong topic khÃ´ng?")

finally:
    spark.stop()