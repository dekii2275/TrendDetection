# scripts/preprocess_data.py
from pathlib import Path
import re
import unicodedata
import os

# N·∫øu sau n√†y v·∫´n mu·ªën fallback sang underthesea th√¨ gi·ªØ l·∫°i import n√†y
# c√≤n hi·ªán t·∫°i ta d√πng VnCoreNLP l√† ch√≠nh.
# from underthesea import word_tokenize

import py_vncorenlp

# scripts/preprocess_data.py (ƒë·∫∑t g·∫ßn ƒë·∫ßu file)

VI_STOPWORDS = {
    # t·ª´ ch·ª©c nƒÉng r·∫•t chung
    "v√†", "trong", "v·ªõi", "c·ªßa", "l√†", "ƒë∆∞·ª£c", "t·∫°i", "t·ª´", "cho", "ƒë·∫øn",
    "n√†y", "kia", "ƒë√≥", "n√†y", "·∫•y", "n√†y", "s·∫Ω", "ƒë√£", "ƒëang", "c≈©ng",
    "nh∆∞ng", "hay", "ho·∫∑c", "n·∫øu", "th√¨", "r·∫±ng", "v√¨", "do", "khi",
    "tr√™n", "d∆∞·ªõi", "gi·ªØa", "sau", "tr∆∞·ªõc", "n∆°i", "n∆°i_ƒë√¢y", "n∆°i_n√†y",

    # ƒë·∫°i t·ª´ / t·ª´ r·∫•t chung trong tin t·ª©c
    "ng∆∞·ªùi", "√¥ng", "b√†", "anh", "ch·ªã", "h·ªç", "ch√∫ng_ta", "ch√∫ng_t√¥i",
    "m·ªôt", "hai", "ba", "nhi·ªÅu", "√≠t", "c√°c", "nh·ªØng", "nhi·ªÅu",
    "nƒÉm", "th√°ng", "ng√†y", "h√¥m_nay", "h√¥m_qua",

    # em c√≥ th·ªÉ th√™m/b·ªõt d·∫ßn khi xem k·∫øt qu·∫£
}


# 1. G·ªëc project (d√πng cho c√°c script kh√°c)
PROJECT_ROOT = Path(__file__).resolve().parents[1]

# 2. Regex ph·ª• tr·ª£ (ƒë·ªÉ d√†nh n·∫øu sau n√†y c·∫ßn)
URL_RE = re.compile(r"https?://\S+")
EMAIL_RE = re.compile(r"\S+@\S+")
NUM_RE = re.compile(r"\d+([\.,]\d+)*")

# Regex lo·∫°i b·ªè d·∫•u c√¢u: . , ; : ! ? " ' ‚Ä¶ ( ) [ ] { } - / v.v.
PUNCT_RE = re.compile(r"[.,;:!?\"'‚Äú‚Äù‚Äò‚Äô()\[\]{}\-\‚Äì‚Äî‚Ä¶/]")

# 3. Pattern nh·∫≠n di·ªán ph·∫ßn ‚Äúƒëu√¥i b√°o / UI‚Äù (Tu·ªïi Tr·∫ª)
TT_FOOTER_PATTERNS = [
    "Chuy·ªÉn sao t·∫∑ng cho th√†nh vi√™n",
    "Tu·ªïi Tr·∫ª Online s·∫Ω g·ªüi ƒë·∫øn b·∫°n",
    "Hi·ªán ch∆∞a c√≥ b√¨nh lu·∫≠n n√†o, h√£y l√† ng∆∞·ªùi ƒë·∫ßu ti√™n b√¨nh lu·∫≠n",
    "Gi·∫•y ph√©p ho·∫°t ƒë·ªông b√°o ƒëi·ªán t·ª≠",
    "¬© Copyright",
    "TuoiTre Online, All rights reserved",
    "Tu·ªïi Tr·∫ª Online gi·ªØ b·∫£n quy·ªÅn",
    "ƒê·ªãa ch·ªâ:",
    "Hotline:",
    "Ph√≤ng Qu·∫£ng C√°o B√°o Tu·ªïi Tr·∫ª",
    "ƒêƒÉng k√Ω email",
    "Th√¥ng tin ƒëƒÉng nh·∫≠p kh√¥ng ƒë√∫ng",
    "T√†i kho·∫£n b·ªã kh√≥a",
    "C√≥ l·ªói ph√°t sinh",
    "M·∫≠t kh·∫©u ph·∫£i c√≥ √≠t nh·∫•t",
    "Vui l√≤ng nh·∫≠p th√¥ng tin v√† √Ω ki·∫øn c·ªßa b·∫°n",
    "Tu·ªïi Tr·∫ª Sao",
    "Th√™m chuy√™n m·ª•c, tƒÉng tr·∫£i nghi·ªám v·ªõiTu·ªïi Tr·∫ª Sao",
    "Tu·ªïi Tr·∫ª Saonh·∫±m t·ª´ng b∆∞·ªõc n√¢ng cao",
]


# ================= VnCoreNLP word segmentation ================= #

SEGMENTER = None  # s·∫Ω load lazy, ch·ªâ 1 l·∫ßn


def get_vncorenlp_segmenter():
    """
    Kh·ªüi t·∫°o VnCoreNLP (ch·ªâ 1 l·∫ßn), d√πng annotator wseg (word segmentation).
    D√πng lu√¥n model ƒë√£ c√≥ s·∫µn trong PROJECT_ROOT / 'vncorenlp'.
    """
    global SEGMENTER
    if SEGMENTER is None:
        # üîπ CH·ªàNH ·ªû ƒê√ÇY: d√πng th∆∞ m·ª•c vncorenlp n·∫±m TRONG project
        save_dir = PROJECT_ROOT / "vncorenlp"

        # N·∫øu mu·ªën an to√†n, c√≥ th·ªÉ t·∫°o th∆∞ m·ª•c (n·∫øu em ch·∫Øc ch·∫Øn ƒë√£ c√≥ r·ªìi th√¨ d√≤ng n√†y kh√¥ng b·∫Øt bu·ªôc)
        os.makedirs(save_dir, exist_ok=True)

        # Kh√¥ng c·∫ßn download_model n·ªØa v√¨ em ƒë√£ c√≥ jar + models
        # N·∫øu mu·ªën v·∫´n c√≥ th·ªÉ b·∫≠t ƒë·ªÉ t·ª± t·∫£i khi thi·∫øu:
        # py_vncorenlp.download_model(save_dir=str(save_dir))

        SEGMENTER = py_vncorenlp.VnCoreNLP(
            annotators=["wseg"],
            save_dir=str(save_dir),
        )
    return SEGMENTER


def word_segment_tokens(text: str) -> list[str]:
    """
    T√°ch t·ª´ ti·∫øng Vi·ªát b·∫±ng VnCoreNLP.

    Tr·∫£ v·ªÅ: list token ph·∫≥ng, v√≠ d·ª•:
    "M√¨nh qu√™ ·ªü Ti·ªÅn Giang." ->
        ["M√¨nh", "qu√™", "·ªü", "Ti·ªÅn_Giang", "."]
    (d·∫•u c√¢u sau ƒë√≥ s·∫Ω b·ªã PUNCT_RE x·ª≠ l√Ω)
    """
    segmenter = get_vncorenlp_segmenter()
    # VnCoreNLP tr·∫£ v·ªÅ list c√°c c√¢u, m·ªói c√¢u l√† chu·ªói c√≥ token c√°ch nhau b·ªüi space
    sentences = segmenter.word_segment(text)
    tokens: list[str] = []
    for sent in sentences:
        tokens.extend(sent.split())
    return tokens


# -------- B∆∞·ªõc 0: C·∫Øt b·ªè footer / boilerplate -------- #
def strip_boilerplate_lines(text: str) -> str:
    """
    Lo·∫°i b·ªè c√°c d√≤ng footer/UI (copyright, Tu·ªïi Tr·∫ª Sao, th√¥ng b√°o l·ªói...).
    N·∫øu g·∫∑p d√≤ng ch·ª©a pattern footer th√¨ d·ª´ng lu√¥n (xem nh∆∞ h·∫øt b√†i).
    """
    if not isinstance(text, str):
        return ""

    lines = text.splitlines()
    cleaned_lines = []
    seen_lines = set()

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # N·∫øu line ch·ª©a 1 trong c√°c pattern footer -> coi nh∆∞ h·∫øt b√†i
        if any(pat in line for pat in TT_FOOTER_PATTERNS):
            break

        # B·ªè d√≤ng l·∫∑p y h·ªát (c√¢u UI l·∫∑p l·∫°i nhi·ªÅu l·∫ßn)
        if line in seen_lines:
            continue
        seen_lines.add(line)

        cleaned_lines.append(line)

    return "\n".join(cleaned_lines)


# -------- B∆∞·ªõc 1: Chu·∫©n h√≥a unicode -------- #
def normalize_unicode(text: str) -> str:
    """
    Chu·∫©n h√≥a unicode v·ªÅ d·∫°ng chu·∫©n NFC (k·∫øt h·ª£p d·∫•u + k√Ω t·ª±).
    Gi√∫p th·ªëng nh·∫•t c√°ch m√£ h√≥a d·∫•u ti·∫øng Vi·ªát.
    """
    if not isinstance(text, str):
        return ""
    return unicodedata.normalize("NFC", text)


# -------- B∆∞·ªõc 2: Chu·∫©n h√≥a c√°ch g√µ d·∫•u ti·∫øng Vi·ªát -------- #
def normalize_vietnamese_diacritics(text: str) -> str:
    """
    Chu·∫©n h√≥a m·ªôt s·ªë c√°ch g√µ d·∫•u ti·∫øng Vi·ªát kh√¥ng chu·∫©n (n·∫øu c√≥).
    ·ªû ƒë√¢y t·∫°m th·ªùi d√πng unicode NFC l√† ƒë·ªß cho ƒëa s·ªë tr∆∞·ªùng h·ª£p.
    N·∫øu ph√°t hi·ªán pattern sai (vd: '√≤a' -> 'o√†') th√¨ c√≥ th·ªÉ b·ªï sung mapping ·ªü ƒë√¢y.
    """
    # Hi·ªán t·∫°i ch·ªâ tr·∫£ l·∫°i text, v√¨ normalize_unicode ƒë√£ x·ª≠ l√Ω c∆° b·∫£n.
    return text


# -------- B∆∞·ªõc 3: Chu·∫©n h√≥a ch·ªØ vi·∫øt th∆∞·ªùng -------- #
def normalize_case(text: str) -> str:
    """
    ƒê∆∞a to√†n b·ªô text v·ªÅ ch·ªØ th∆∞·ªùng.
    """
    return text.lower()


# -------- B∆∞·ªõc 4 & 5: T√°ch t·ª´, ƒë∆∞a v·ªÅ 1 d√≤ng -------- #
def clean_and_tokenize(text: str) -> str:
    """
    - ƒê∆∞a c·∫£ ƒëo·∫°n vƒÉn v·ªÅ tr√™n 1 d√≤ng
    - X√≥a c√°c kho·∫£ng c√°ch th·ª´a
    - Lo·∫°i b·ªè d·∫•u c√¢u (. , ; : ! ? ‚Ä¶, ngo·∫∑c, g·∫°ch n·ªëi, ...)
    - T√°ch t·ª´ ti·∫øng Vi·ªát b·∫±ng VnCoreNLP

    KH√îNG x√≥a s·ªë, KH√îNG l·ªçc stopword.
    M·ª•c ti√™u: ch·ªâ chu·∫©n h√≥a, kh√¥ng l√†m m·∫•t th√¥ng tin n·ªôi dung.
    """
    # Chuy·ªÉn xu·ªëng d√≤ng, tab th√†nh space
    text = re.sub(r"[\r\n\t]", " ", text)

    # T√°ch t·ª´ tr∆∞·ªõc r·ªìi m·ªõi x·ª≠ l√Ω d·∫•u c√¢u cho ch·∫Øc ch·∫Øn
    # (v√¨ VnCoreNLP d√πng d·∫•u ch·∫•m ƒë·ªÉ ph√¢n c√¢u)
    tokens = word_segment_tokens(text)

    # B·ªè d·∫•u c√¢u kh·ªèi t·ª´ng token (n·∫øu mu·ªën gi·ªØ s·ªë, ch·ªØ)
    cleaned_tokens: list[str] = []
    for tok in tokens:
        # thay d·∫•u c√¢u trong token b·∫±ng space r·ªìi gom l·∫°i
        tok_no_punct = PUNCT_RE.sub(" ", tok)
        # c√≥ th·ªÉ sinh ra nhi·ªÅu space -> t√°ch l·∫°i
        for sub in tok_no_punct.split():
            cleaned_tokens.append(sub)

    # Gom nhi·ªÅu kho·∫£ng tr·∫Øng b·∫±ng c√°ch join l·∫°i = 1 space
    return " ".join(cleaned_tokens)


# -------- H√†m ch√≠nh d√πng trong to√†n project -------- #
def preprocess_text(raw_text: str) -> str:
    """
    Pipeline ti·ªÅn x·ª≠ l√Ω ho√†n ch·ªânh, theo 5 b∆∞·ªõc:

    0. (Th√™m) C·∫Øt b·ªè footer/UI c·ªßa b√°o (Tu·ªïi Tr·∫ª).
    1. Chu·∫©n h√≥a unicode
    2. Chu·∫©n h√≥a c√°ch g√µ d·∫•u ti·∫øng Vi·ªát
    3. Chu·∫©n h√≥a ch·ªØ vi·∫øt th∆∞·ªùng
    4. T√°ch t·ª´ ti·∫øng Vi·ªát (VnCoreNLP)
    5. ƒê∆∞a c·∫£ ƒëo·∫°n vƒÉn v·ªÅ tr√™n 1 d√≤ng, x√≥a c√°c kho·∫£ng c√°ch th·ª´a,
       lo·∫°i b·ªè d·∫•u c√¢u.
    """
    # B∆∞·ªõc 0: b·ªè footer/UI
    text = strip_boilerplate_lines(raw_text)

    # B∆∞·ªõc 1: unicode NFC
    text = normalize_unicode(text)

    # B∆∞·ªõc 2: chu·∫©n h√≥a c√°ch g√µ d·∫•u (hi·ªán ch∆∞a s·ª≠a g√¨ th√™m)
    text = normalize_vietnamese_diacritics(text)

    # B∆∞·ªõc 3: ch·ªØ th∆∞·ªùng
    text = normalize_case(text)

    # B∆∞·ªõc 4 + 5: t√°ch t·ª´ + b·ªè d·∫•u c√¢u + gom 1 d√≤ng
    text = clean_and_tokenize(text)

    return text
