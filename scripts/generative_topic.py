import sys
import json
import pandas as pd
import google.generativeai as genai
from bertopic import BERTopic
from bertopic.representation import BaseRepresentation
from bertopic.representation._utils import truncate_document
from typing import Mapping, List, Tuple
from sentence_transformers import SentenceTransformer
from scipy.sparse import csr_matrix


GOOGLE_API_KEY = "AIzaSyDr4lo9fMpkgBbl0a8rj7dFDFDeQXdIwks"


# ============================================================
# 1) GEMINI BACKEND
# ============================================================
class GeminiBackend(BaseRepresentation):
    def __init__(
        self,
        client: str,
        model_name: str,
        prompt: str,
        nr_docs: int = 4,
        diversity: float | None = None,
        doc_length: int | None = 200,
        tokenizer: str | None = "whitespace",
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.api_key = client
        self.model_name = model_name
        self.prompt = prompt
        self.nr_docs = nr_docs
        self.diversity = diversity
        self.doc_length = doc_length
        self.tokenizer = tokenizer

        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel(self.model_name)

    def extract_topics(
        self,
        topic_model,
        documents: pd.DataFrame,
        c_tf_idf: csr_matrix,
        topics: Mapping[int, List[Tuple[str, float]]],
    ) -> Mapping[int, List[Tuple[str, float]]]:

        print(f"\n--- G·ªåI GEMINI ({len(topics)} topic) ---")

        repr_docs_mappings, _, _, _ = topic_model._extract_representative_docs(
            c_tf_idf, documents, topics,
            500, self.nr_docs, self.diversity
        )

        updated_topics = {}

        for topic_id, docs in repr_docs_mappings.items():
            if topic_id == -1:
                continue

            topic_words = topics[topic_id]
            keywords_text = ", ".join([w for w, _ in topic_words][:10])

            truncated_docs = [
                truncate_document(topic_model, self.doc_length, self.tokenizer, doc)
                for doc in docs
            ]
            docs_text = "\n".join(f"- {d}" for d in truncated_docs)

            final_prompt = (
                self.prompt
                .replace("[KEYWORDS]", keywords_text)
                .replace("[DOCUMENTS]", docs_text)
            )

            # G·ªçi Gemini
            label = "Kh√¥ng r√µ"
            try:
                res = self.model.generate_content(final_prompt)
                if getattr(res, "text", None):
                    label = res.text.strip().replace("\n", " ")
                print("  ‚Üí Gemini ƒë·∫∑t t√™n:", label)
            except Exception as e:
                print("  ‚ùå L·ªói Gemini:", e)

            updated_topics[topic_id] = [(label, 1.0)] + topic_words

        return updated_topics


# ============================================================
# 2) PIPELINE CH√çNH
# ============================================================
def run_pipeline(docs: list[str], topic_name: str):

    print(f"\nüî• CH·∫†Y PIPELINE CHO TOPIC: {topic_name}")
    print(f" ‚Üí S·ªë b√†i b√°o: {len(docs)}")

    # =====================================================
    #  SKIP T·ª™ TRONG generative_topic.py
    # =====================================================
    if len(docs) <= 5:
        print(f"‚ùå B·ªé QUA TOPIC '{topic_name}' ‚Äî QU√Å √çT B√ÄI ({len(docs)} / 5)")
        return None

    # Load embedding model
    try:
        embedding_model = SentenceTransformer("VoVanPhuc/sup-SimCSE-VietNamese-phobert-base")
    except:
        print("‚ö† Kh√¥ng t·∫£i ƒë∆∞·ª£c model ti·∫øng Vi·ªát ‚Üí fallback sang all-MiniLM-L6-v2")
        embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

    prompt = """
    B·∫°n l√† bi√™n t·∫≠p vi√™n b√°o t·∫°i Vi·ªát Nam.

    Nhi·ªám v·ª•: ƒê·∫∂T M·ªòT TI√äU ƒê·ªÄ XU H∆Ø·ªöNG cho nh√≥m b√†i b√°o d∆∞·ªõi ƒë√¢y.

    Y√äU C·∫¶U:
    - Ti·∫øng Vi·ªát, c√≥ d·∫•u.
    - Ng·∫Øn g·ªçn (3‚Äì7 t·ª´).
    - D·∫°ng danh t·ª´.
    - Kh√¥ng d√πng markdown ho·∫∑c ngo·∫∑c k√©p.

    T·ª´ kh√≥a: [KEYWORDS]
    M·ªôt v√†i ƒëo·∫°n tin ti√™u bi·ªÉu:
    [DOCUMENTS]

    Ch·ªâ in RA DUY NH·∫§T ti√™u ƒë·ªÅ xu h∆∞·ªõng:
    """

    backend = GeminiBackend(
        client=GOOGLE_API_KEY,
        model_name="gemini-2.5-flash",
        prompt=prompt
    )

    # UMAP s·∫Ω l·ªói n·∫øu docs qu√° √≠t ‚Üí t·∫Øt gi·∫£m chi·ªÅu
    model = BERTopic(
        embedding_model=embedding_model,
        representation_model=backend,
        umap_model=None,       # x·ª≠ l√Ω √≠t sample r·∫•t an to√†n
        hdbscan_model=None,    # d√πng KMeans thay cho HDBSCAN ƒë·ªÉ kh√¥ng l·ªói
        min_topic_size=2
    )

    topics, _ = model.fit_transform(docs)

    info = model.get_topic_info()
    main_topic = info.iloc[1]   # h√†ng 0 = outlier -1

    trend_title = main_topic["Name"]

    print(f"\nüéØ K·∫æT QU·∫¢ CU·ªêI ‚Äî TOPIC: {topic_name}")
    print(" ‚Üí Ti√™u ƒë·ªÅ xu h∆∞·ªõng:", trend_title)

    return trend_title


# ============================================================
# 3) MAIN ‚Äî NH·∫¨N INPUT T·ª™ SPARK
# ============================================================
if __name__ == "__main__":
    docs = json.loads(sys.argv[1])
    topic_name = sys.argv[2]
    run_pipeline(docs, topic_name)
