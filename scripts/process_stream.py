from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col,
    from_json,
    from_unixtime,
    to_timestamp,
    window,
    collect_list,
    date_format,
    when,
    count,
)
from pyspark.sql.types import StructType, StringType, LongType
import subprocess
import json

# ============================================================
# 1. SCHEMA STREAM
# ============================================================
schema = (
    StructType()
    .add("title", StringType())
    .add("timestamp", LongType())
    .add("topic", StringType())
)

spark = (
    SparkSession.builder.appName("WindowedTrendingNews")
    .config("spark.sql.shuffle.partitions", "4")
    .getOrCreate()
)

spark.sparkContext.setLogLevel("WARN")

# ============================================================
# 2. READ FROM KAFKA STREAM - ƒê√É FIX
# ============================================================
df = (
    spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers", "localhost:9092")
    .option("subscribe", "news_raw")
    .option("startingOffsets", "latest")
    # üî• FIX: Th√™m option n√†y ƒë·ªÉ kh√¥ng fail khi m·∫•t data
    .option("failOnDataLoss", "false")
    # üî• FIX: TƒÉng timeout n·∫øu Kafka ch·∫≠m
    .option("kafkaConsumer.pollTimeoutMs", "10000")
    .load()
)

parsed = (
    df.selectExpr("CAST(value AS STRING)")
    .select(from_json(col("value"), schema).alias("data"))
    .select("data.*")
)

# ============================================================
# 2.1 CHU·∫®N H√ìA event_time - FIX LOGIC
# ============================================================
# Timestamp trong data ƒë√£ ·ªü d·∫°ng milliseconds, c·∫ßn chia cho 1000 ƒë·ªÉ c√≥ seconds
normalized_ts = col("timestamp") / 1000

parsed = parsed.withColumn("event_time", to_timestamp(from_unixtime(normalized_ts)))

# ============================================================
# 3. WINDOWED AGGREGATION - C·∫¢I THI·ªÜN LOGIC
# ============================================================
# Th√™m filter ƒë·ªÉ ch·ªâ l·∫•y d·ªØ li·ªáu h·ª£p l·ªá
filtered_parsed = parsed.filter(
    col("event_time").isNotNull() & 
    col("title").isNotNull() & 
    col("topic").isNotNull()
)

windowed = (
    filtered_parsed.withWatermark("event_time", "10 minutes")  # Gi·∫£m watermark
    .groupBy(
        window(col("event_time"), "15 minutes", "5 minutes"),  # Gi·∫£m window size
        col("topic"),
    )
    .agg(
        collect_list("title").alias("docs"),
        count("*").alias("doc_count")  # Th√™m count ƒë·ªÉ debug
    )
    .select(
        col("topic"),
        col("docs"),
        col("doc_count"),
        date_format(col("window.start"), "yyyy-MM-dd HH:mm:ss").alias("win_start"),
        date_format(col("window.end"), "yyyy-MM-dd HH:mm:ss").alias("win_end"),
    )
    .filter(col("doc_count") > 0)  # Ch·ªâ l·∫•y window c√≥ d·ªØ li·ªáu
)

# ============================================================
# 4. PROCESS M·ªñI WINDOW (foreachBatch)
# ============================================================


def process_window(batch_df, batch_id):
    print(f"\n================= WINDOW BATCH {batch_id} =================")

    # Debug: Ki·ªÉm tra batch_df tr∆∞·ªõc khi collect
    print(f"Batch DataFrame c√≥ {batch_df.count()} d√≤ng")
    
    rows = batch_df.collect()
    if not rows:
        print("‚Üí Batch r·ªóng sau collect()")
        return

    print(f"‚Üí C√≥ {len(rows)} nh√≥m topic trong batch")

    for row in rows:
        topic = row["topic"]
        docs = row["docs"]
        doc_count = row["doc_count"]
        win_start = row["win_start"]
        win_end = row["win_end"]

        print(f"\n--- WINDOW TOPIC: {topic} ---")
        print(f"S·ªë b√†i trong c·ª≠a s·ªï: {doc_count} (docs list length: {len(docs) if docs else 0})")
        print(f"C·ª≠a s·ªï: {win_start} ‚Üí {win_end}")

        # Ki·ªÉm tra docs kh√¥ng r·ªóng v√† c√≥ ƒë·ªß b√†i
        if not docs or len(docs) < 3:  # Gi·∫£m threshold t·ª´ 5 xu·ªëng 3
            print(f"‚Üí B·ªè qua topic {topic} (qu√° √≠t b√†i: {len(docs) if docs else 0})")
            continue

        print(f"‚Üí Topic {topic} c√≥ ƒë·ªß b√†i! G·ªçi generative_topic.py ...")
        print(f"‚Üí Danh s√°ch c√°c title: {docs[:3]}...")  # In v√†i title ƒë·∫ßu ƒë·ªÉ debug

        try:
            cmd = [
                "python3",
                "/home/lok/dev/projects/Parallel_computing/TrendDetection/scripts/generative_topic.py",
                json.dumps(docs, ensure_ascii=False),  # Th√™m ensure_ascii=False cho ti·∫øng Vi·ªát
                topic,
            ]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)

            if result.returncode == 0:
                print("‚úÖ X·ª≠ l√Ω th√†nh c√¥ng!")
                if result.stdout:
                    print(">>> OUTPUT:")
                    print(result.stdout)
            else:
                print(f"‚ùå L·ªói v·ªõi exit code: {result.returncode}")
                if result.stderr:
                    print(">>> STDERR:")
                    print(result.stderr)

        except subprocess.TimeoutExpired:
            print("‚è∞ Timeout khi ch·∫°y generative_topic.py")
        except Exception as e:
            print(f"‚ùå L·ªói exception: {e}")


# ============================================================
# 5. G·∫ÆN FOREACHBATCH + CHECKPOINT
# ============================================================
query = (
    windowed.writeStream.foreachBatch(process_window)
    .option(
        "checkpointLocation",
        "/home/lok/dev/projects/Parallel_computing/TrendDetection/scripts/checkpoint_windowed",
    )
    .outputMode("update")
    .start()
)

query.awaitTermination()
